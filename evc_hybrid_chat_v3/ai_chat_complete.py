"""
üöÄ Complete AI Chat System with Web Search & Context Memory
- Web search integration
- Context-aware conversation
- EVC emotion tracking
- Full working system
"""

import os
import json
import time
import yaml
import requests
import streamlit as st
from datetime import datetime
from typing import Dict, List, Any, Tuple
from collections import deque

# ============================================================
# 1. WEB SEARCH MODULE
# ============================================================

class WebSearcher:
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ DuckDuckGo API"""
    
    def __init__(self):
        self.base_url = "https://duckduckgo.com/api"
        self.timeout = 10
    
    def search(self, query: str, max_results: int = 3) -> List[Dict[str, str]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö
        Returns: [{"title": "...", "url": "...", "snippet": "..."}, ...]
        """
        try:
            # ‡πÉ‡∏ä‡πâ DuckDuckGo search
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
            }
            
            params = {
                'q': query,
                'format': 'json'
            }
            
            response = requests.get(
                self.base_url,
                params=params,
                headers=headers,
                timeout=self.timeout
            )
            
            if response.status_code != 200:
                return []
            
            data = response.json()
            results = []
            
            # Parse DuckDuckGo results
            for result in data.get('Results', [])[:max_results]:
                results.append({
                    'title': result.get('Title', 'N/A'),
                    'url': result.get('FirstURL', 'N/A'),
                    'snippet': result.get('Text', 'N/A')
                })
            
            # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏≤‡∏Å Results ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å RelatedTopics
            if not results:
                for topic in data.get('RelatedTopics', [])[:max_results]:
                    if 'Text' in topic:
                        results.append({
                            'title': topic.get('Text', 'N/A')[:50],
                            'url': topic.get('FirstURL', ''),
                            'snippet': topic.get('Text', 'N/A')
                        })
            
            return results
        
        except Exception as e:
            print(f"‚ùå Web search error: {str(e)}")
            return []
    
    def format_search_results(self, results: List[Dict]) -> str:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô text format"""
        if not results:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"
        
        formatted = "üìä ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:\n\n"
        for i, result in enumerate(results, 1):
            formatted += f"{i}. **{result['title']}**\n"
            formatted += f"   üîó {result['url']}\n"
            formatted += f"   üìù {result['snippet'][:150]}...\n\n"
        
        return formatted


# ============================================================
# 2. CONVERSATION MEMORY MODULE
# ============================================================

class ConversationMemory:
    """‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"""
    
    def __init__(self, max_turns: int = 100):
        self.turns = deque(maxlen=max_turns)
        self.full_history = []
        self.search_cache = {}  # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    
    def add_turn(self, user_query: str, ai_response: str, 
                 search_used: bool = False, search_query: str = "",
                 evc_state: Dict = None):
        """‡πÄ‡∏û‡∏¥‡πà‡∏° turn ‡πÉ‡∏´‡∏°‡πà"""
        turn = {
            "turn_number": len(self.full_history) + 1,
            "timestamp": datetime.now().isoformat(),
            "user_query": user_query,
            "ai_response": ai_response,
            "search_used": search_used,
            "search_query": search_query,
            "evc_state": evc_state or {}
        }
        
        self.turns.append(turn)
        self.full_history.append(turn)
    
    def get_context(self, recent_n: int = 10) -> str:
        """‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ N turn ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        recent = list(self.turns)[-recent_n:]
        
        context = "<conversation_history>\n"
        for turn in recent:
            context += f"  <turn number=\"{turn['turn_number']}\">\n"
            context += f"    <user>{turn['user_query'][:150]}</user>\n"
            context += f"    <assistant>{turn['ai_response'][:200]}</assistant>\n"
            if turn['search_used']:
                context += f"    <search_info>{turn['search_query']}</search_info>\n"
            context += f"  </turn>\n"
        context += "</conversation_history>\n"
        
        return context
    
    def cache_search(self, query: str, results: List[Dict]):
        """‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ã‡πâ‡∏≥"""
        self.search_cache[query.lower()] = {
            'results': results,
            'timestamp': datetime.now().isoformat()
        }
    
    def get_cached_search(self, query: str) -> List[Dict] | None:
        """‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÅ‡∏Ñ‡∏ä (‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)"""
        cache_key = query.lower()
        if cache_key in self.search_cache:
            cached = self.search_cache[cache_key]
            cache_time = datetime.fromisoformat(cached['timestamp'])
            if (datetime.now() - cache_time).seconds < 3600:  # 1 hour
                return cached['results']
        return None


# ============================================================
# 3. FIXED CONTEXT BUILDER
# ============================================================

class ContextBuilder:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á system prompt ‡∏ó‡∏µ‡πà‡∏°‡∏µ continuity"""
    
    @staticmethod
    def build_system_prompt(
        conversation_memory: ConversationMemory,
        current_turn: int,
        ai_name: str,
        user_query: str,
        evc_state: Dict,
        web_context: str = "",
        mode: str = "chat"
    ) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á system prompt ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"""
        
        # ‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
        history = conversation_memory.get_context(recent_n=8)
        
        # Mandatory instructions
        mandatory = """
<MANDATORY>
‚úÖ RULES:
1. ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤ ("‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏≤‡∏°...", "‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏µ‡πâ...")
2. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤..."
3. ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏≤‡∏°‡∏ã‡πâ‡∏≥ ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
4. ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
</MANDATORY>
"""
        
        # EVC tone
        phase_tone = {
            "calm": "‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏õ‡∏Å‡∏ï‡∏¥",
            "focus": "‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô",
            "overheat": "‡πÉ‡∏à‡πÄ‡∏¢‡πá‡∏ô ‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á",
            "fear": "‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à ‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô",
            "cooldown": "‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô ‡∏ä‡∏≤‡∏ç‡∏â‡∏•‡∏≤‡∏î"
        }.get(evc_state.get('phase', 'calm'), "‡∏õ‡∏Å‡∏ï‡∏¥")
        
        evc_section = f"""
EVC State: E={evc_state.get('E', 0.5):.2f}, K={evc_state.get('K', 0.45):.2f}, Phase={evc_state.get('phase', 'calm')}
Tone: {phase_tone}
"""
        
        # Web context
        web_section = f"\nWEB SEARCH RESULTS:\n{web_context}" if web_context else ""
        
        system_prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ {ai_name} - ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÑ‡∏î‡πâ ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ

{mandatory}

{evc_section}

{history}

{web_section}

---
Turn {current_turn}: {user_query}

üëâ ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏û‡∏π‡∏î‡∏°‡∏≤
"""
        
        return system_prompt


# ============================================================
# 4. CORE LLM MODULE (Enhanced)
# ============================================================

class EnhancedLLM:
    """LLM ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö web search ‡πÅ‡∏•‡∏∞ context"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.provider = os.getenv("LLM_PROVIDER", "offline").lower()
        self.base_url = os.getenv("OPENAI_BASE_URL", "")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.api_key = os.getenv("OPENAI_API_KEY", "")
        self.timeout = 30
        
        # Load config
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        except:
            self.config = {}
        
        self.searcher = WebSearcher()
        self.memory = ConversationMemory()
    
    def _should_search(self, query: str) -> bool:
        """‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        search_keywords = [
            "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", "‡∏´‡∏≤", "‡∏ö‡∏≠‡∏Å", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö",
            "‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£", "‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà",
            "‡πÉ‡∏´‡∏°‡πà", "‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î", "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ", "‡∏Ç‡πà‡∏≤‡∏ß", "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"
        ]
        return any(keyword in query.lower() for keyword in search_keywords)
    
    def _call_llm(self, system_prompt: str, user_query: str) -> str:
        """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM API"""
        try:
            if not self.base_url or self.provider == "offline":
                return "‚ö†Ô∏è LLM offline - ‡πÉ‡∏ä‡πâ Ollama ‡∏´‡∏£‡∏∑‡∏≠ API key ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}" if self.api_key else ""
            }
            
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query}
                ],
                "temperature": 0.7,
                "max_tokens": 512,
                "stream": False
            }
            
            # Try Ollama first
            response = requests.post(
                f"{self.base_url}/api/chat" if "localhost" in self.base_url else f"{self.base_url}/v1/chat/completions",
                json=payload,
                headers=headers,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                data = response.json()
                if "message" in data:
                    return data["message"]["content"]
                elif "choices" in data:
                    return data["choices"][0]["message"]["content"]
            
            return f"‚ö†Ô∏è LLM error {response.status_code}"
        
        except Exception as e:
            return f"‚ö†Ô∏è Connection error: {str(e)[:60]}"
    
    def generate_response(self, user_query: str, evc_state: Dict = None) -> Tuple[str, Dict]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        Returns: (response, metadata)
        """
        
        if evc_state is None:
            evc_state = {"E": 0.5, "K": 0.45, "phase": "calm"}
        
        search_used = False
        search_results = []
        web_context = ""
        
        # 1. ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        if self._should_search(user_query):
            cached = self.memory.get_cached_search(user_query)
            
            if cached:
                search_results = cached
            else:
                search_results = self.searcher.search(user_query, max_results=3)
            
            if search_results:
                search_used = True
                web_context = self.searcher.format_search_results(search_results)
        
        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á system prompt
        current_turn = len(self.memory.full_history) + 1
        system_prompt = ContextBuilder.build_system_prompt(
            conversation_memory=self.memory,
            current_turn=current_turn,
            ai_name="Assistant",
            user_query=user_query,
            evc_state=evc_state,
            web_context=web_context,
            mode="chat"
        )
        
        # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
        response = self._call_llm(system_prompt, user_query)
        
        # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
        self.memory.add_turn(
            user_query=user_query,
            ai_response=response,
            search_used=search_used,
            search_query=user_query if search_used else "",
            evc_state=evc_state
        )
        
        if search_used and search_results:
            self.memory.cache_search(user_query, search_results)
        
        metadata = {
            "search_used": search_used,
            "search_results_count": len(search_results),
            "turn_number": current_turn,
            "web_context_used": bool(web_context)
        }
        
        return response, metadata
    
    def get_conversation_history(self) -> List[Dict]:
        """‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        return self.memory.full_history
    
    def export_conversation(self, filepath: str):
        """‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.memory.full_history, f, ensure_ascii=False, indent=2)


# ============================================================
# 5. STREAMLIT WEB UI
# ============================================================

def main():
    """Streamlit Web Interface"""
    
    st.set_page_config(
        page_title="ü§ñ AI Chat with Web Search",
        page_icon="üîç",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ü§ñ AI Chat System with Web Search")
    st.caption("‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ä‡∏ó‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÑ‡∏î‡πâ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á")
    
    # ===== Initialize Session State =====
    if "llm" not in st.session_state:
        st.session_state.llm = EnhancedLLM()
    
    if "evc_state" not in st.session_state:
        st.session_state.evc_state = {"E": 0.5, "K": 0.45, "phase": "calm"}
    
    # ===== Sidebar =====
    with st.sidebar:
        st.subheader("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
        
        # LLM Status
        provider = os.getenv("LLM_PROVIDER", "offline")
        model = os.getenv("OPENAI_MODEL", "unknown")
        
        if provider == "offline":
            st.error(f"üî¥ Offline Mode")
        else:
            st.success(f"üü¢ {provider.upper()}\nModel: {model}")
        
        st.divider()
        
        # EVC Display
        st.subheader("üìä EVC Status")
        col1, col2, col3 = st.columns(3)
        col1.metric("E (Energy)", f"{st.session_state.evc_state['E']:.2f}")
        col2.metric("K (Stability)", f"{st.session_state.evc_state['K']:.2f}")
        col3.metric("Phase", st.session_state.evc_state['phase'].upper())
        
        st.divider()
        
        # Conversation Stats
        st.subheader("üìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥")
        history = st.session_state.llm.get_conversation_history()
        st.metric("‡∏£‡∏ß‡∏° Turns", len(history))
        
        if history:
            search_count = sum(1 for h in history if h.get('search_used', False))
            st.metric("‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", search_count)
        
        st.divider()
        
        # Export
        if st.button("üíæ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", use_container_width=True):
            filepath = f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            st.session_state.llm.export_conversation(filepath)
            st.success(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ: {filepath}")
    
    # ===== Main Chat Area =====
    col_chat, col_history = st.columns([2.5, 1.5])
    
    with col_chat:
        st.subheader("üí¨ ‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
        
        # Chat input
        with st.form(key="chat_form", clear_on_submit=True):
            user_input = st.text_area(
                "‡∏û‡∏π‡∏î‡∏Å‡∏±‡∏ö AI:",
                placeholder="‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å...",
                height=100
            )
            
            col_btn1, col_btn2 = st.columns(2)
            with col_btn1:
                submit = st.form_submit_button("üì§ ‡∏™‡πà‡∏á", use_container_width=True)
            with col_btn2:
                search_mode = st.checkbox("üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", value=True)
        
        # Process input
        if submit and user_input.strip():
            with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•..."):
                response, metadata = st.session_state.llm.generate_response(
                    user_query=user_input,
                    evc_state=st.session_state.evc_state
                )
                
                # Display response
                st.success("‚úÖ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")
                st.markdown(f"**ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:**\n\n{response}")
                
                # Show metadata
                with st.expander("üìä ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"):
                    st.json(metadata)
                
                st.divider()
        
        # Display chat history
        history = st.session_state.llm.get_conversation_history()
        if history:
            for i, turn in enumerate(reversed(history[-5:]), 1):
                with st.container():
                    st.markdown(f"**Turn {len(history) - i + 1}**")
                    st.markdown(f"üßç **‡∏Ñ‡∏∏‡∏ì:** {turn['user_query']}")
                    st.markdown(f"ü§ñ **AI:** {turn['ai_response'][:300]}...")
                    
                    if turn.get('search_used'):
                        st.info(f"üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {turn['search_query']}")
                    
                    st.divider()
    
    with col_history:
        st.subheader("üìã ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥")
        
        history = st.session_state.llm.get_conversation_history()
        
        if history:
            st.metric("‡∏£‡∏ß‡∏° Turns", len(history))
            
            with st.expander("üìñ ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"):
                for i, turn in enumerate(history[-10:], 1):
                    st.write(f"**Turn {len(history) - 10 + i}:**")
                    st.write(f"Q: {turn['user_query'][:50]}...")
                    if turn.get('search_used'):
                        st.write("‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
                    st.write("---")
        else:
            st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")


if __name__ == "__main__":
    main()