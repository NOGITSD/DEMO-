#!/usr/bin/env python3
"""
Setup ‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö Ollama ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö EVC Hybrid Chat
"""

import os
import sys
import requests
import subprocess
import time

def check_ollama_running():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    try:
        response = requests.get('http://localhost:11434', timeout=2)
        return response.status_code == 200
    except:
        return False

def start_ollama():
    """‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏£‡∏¥‡πà‡∏° Ollama"""
    print("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏° Ollama...")
    try:
        if sys.platform == "win32":
            subprocess.Popen(["ollama", "serve"], 
                           creationflags=subprocess.CREATE_NEW_CONSOLE)
        else:
            subprocess.Popen(["ollama", "serve"], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
        time.sleep(3)
        return check_ollama_running()
    except FileNotFoundError:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á 'ollama' ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å https://ollama.ai/download")
        return False
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return False

def list_models():
    """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß"""
    try:
        result = subprocess.run(["ollama", "list"], 
                              capture_output=True, 
                              text=True, 
                              timeout=5)
        return result.stdout
    except:
        return None

def pull_model(model_name):
    """‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model"""
    print(f"\nüì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {model_name}...")
    print("‚è≥ ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà (‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏ô‡πá‡∏ï)")
    try:
        subprocess.run(["ollama", "pull", model_name], check=True)
        print(f"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {model_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        return True
    except subprocess.CalledProcessError:
        print(f"‚ùå ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {model_name} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß")
        return False
    except FileNotFoundError:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á 'ollama'")
        return False

def test_model(model_name):
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö model"""
    print(f"\nüß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö {model_name}...")
    try:
        response = requests.post(
            'http://localhost:11434/api/chat',
            json={
                "model": model_name,
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant. Respond in Thai."},
                    {"role": "user", "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡πà‡∏≠‡∏¢"}
                ],
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 100
                }
            },
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            message = data.get("message", {}).get("content", "")
            print(f"‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n")
            print(f"üìù ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å {model_name}:")
            print("-" * 60)
            print(message)
            print("-" * 60)
            return True
        else:
            print(f"‚ùå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß (Status: {response.status_code})")
            return False
            
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return False

def create_env_file(model_name):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .env"""
    env_content = f"""# EVC Hybrid Chat Configuration
LLM_PROVIDER=ollama
OPENAI_BASE_URL=http://localhost:11434
OPENAI_MODEL={model_name}
DEBUG_LOG=true
"""
    
    with open(".env", "w", encoding="utf-8") as f:
        f.write(env_content)
    
    print(f"\n‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .env ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢ (model: {model_name})")

def main():
    print("=" * 60)
    print("ü§ñ EVC Hybrid Chat - Ollama Setup Wizard")
    print("=" * 60)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ollama
    print("\nüì° ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ollama...")
    if not check_ollama_running():
        print("‚ö†Ô∏è  Ollama ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà")
        if not start_ollama():
            print("\n‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° Ollama ‡πÑ‡∏î‡πâ")
            print("\nüìñ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:")
            print("1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama ‡∏à‡∏≤‡∏Å: https://ollama.ai/download")
            print("2. ‡πÄ‡∏õ‡∏¥‡∏î Terminal/CMD ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô: ollama serve")
            print("3. ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
            return
    
    print("‚úÖ Ollama ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models
    print("\nüìã Models ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß:")
    models_output = list_models()
    if models_output:
        print(models_output)
    else:
        print("  (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ models)")
    
    # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ models ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    print("\nüí° Models ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:")
    recommended = [
        ("llama3:8b", "‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ RAM 8GB+", "~4.7GB"),
        ("qwen2:7b", "‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢", "~4.4GB"),
        ("llama2:7b", "‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤, ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£", "~3.8GB"),
        ("phi3:mini", "‡πÄ‡∏•‡πá‡∏Å, ‡πÄ‡∏£‡πá‡∏ß (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏û‡∏≠‡πÉ‡∏ä‡πâ)", "~2.3GB")
    ]
    
    for i, (model, desc, size) in enumerate(recommended, 1):
        print(f"  {i}. {model:15} - {desc:30} [{size}]")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model
    print("\n‚ùì ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á model ‡πÑ‡∏´‡∏ô?")
    choice = input("   ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (1-4) ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ä‡∏∑‡πà‡∏≠ model ‡πÄ‡∏≠‡∏á [1]: ").strip()
    
    if not choice:
        choice = "1"
    
    if choice.isdigit() and 1 <= int(choice) <= 4:
        model_name = recommended[int(choice) - 1][0]
    else:
        model_name = choice
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ model ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if models_output and model_name in models_output:
        print(f"\n‚úÖ ‡∏°‡∏µ {model_name} ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
        skip = input("   ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î? (y/n) [y]: ").strip().lower()
        if skip != 'n':
            print("‚è≠Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î")
        else:
            pull_model(model_name)
    else:
        if not pull_model(model_name):
            return
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö model
    print("\n" + "=" * 60)
    if not test_model(model_name):
        print("\n‚ö†Ô∏è  ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á .env
    print("\n" + "=" * 60)
    create = input("‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .env ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥? (y/n) [y]: ").strip().lower()
    if create != 'n':
        create_env_file(model_name)
    
    # ‡∏™‡∏£‡∏∏‡∏õ
    print("\n" + "=" * 60)
    print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    print("=" * 60)
    print("\nüöÄ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ:")
    print("1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå .env")
    print("2. ‡∏£‡∏±‡∏ô: streamlit run app.py")
    print("3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô EVC Hybrid Chat!")
    print("\nüí° Tips:")
    print("- ‡∏ñ‡πâ‡∏≤‡∏ï‡∏≠‡∏ö‡∏ä‡πâ‡∏≤ ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô model ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤")
    print("- ‡∏ñ‡πâ‡∏≤‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏î‡∏µ ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô model ‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤")
    print("- ‡∏ñ‡πâ‡∏≤‡∏´‡∏°‡∏î‡πÅ‡∏£‡∏° ‡∏õ‡∏¥‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏Å‡πà‡∏≠‡∏ô")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á")
    except Exception as e:
        print(f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {e}")
        import traceback
        traceback.print_exc()