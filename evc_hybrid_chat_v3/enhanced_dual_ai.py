#!/usr/bin/env python3
"""
Enhanced Dual AI Conversation Engine with Full Memory & Context
- ‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô Long Context Conversation
- ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ
- Built-in summarization for very long contexts
"""

import os, json, time, yaml
from datetime import datetime
from typing import Dict, List, Any, Tuple
from collections import deque

from evc_engine import EVCEngine
from reflection import reflect
from core_llm import get_llm_core


class ConversationMemory:
    """Store full conversation with efficient retrieval"""
    
    def __init__(self, max_turns: int = 100):
        self.turns = deque(maxlen=max_turns)  # Last 100 turns
        self.full_history = []  # All turns (for archive)
        self.summary = ""
        self.key_points = []
    
    def add_turn(self, speaker: str, message: str, response: str, evc_state: Dict):
        """Add turn to memory"""
        turn = {
            "timestamp": datetime.now().isoformat(),
            "speaker": speaker,
            "message": message,
            "response": response,
            "evc_state": evc_state
        }
        self.turns.append(turn)
        self.full_history.append(turn)
    
    def get_context(self, recent_n: int = 20) -> str:
        """Get formatted context from recent turns"""
        recent = list(self.turns)[-recent_n:]
        context = "=== CONVERSATION HISTORY ===\n\n"
        
        for turn in recent:
            speaker = turn["speaker"]
            msg = turn["message"]
            resp = turn["response"]
            evc = turn["evc_state"]
            
            context += f"[{speaker}] Query:\n{msg}\n\n"
            context += f"[{speaker}'s Response]:\n{resp}\n"
            context += f"EVC State: E={evc.get('E', 0.5):.2f}, K={evc.get('K', 0.45):.2f}, Phase={evc.get('phase', 'calm')}\n"
            context += "-" * 60 + "\n\n"
        
        return context
    
    def get_summary_context(self) -> str:
        """Get summary if history is very long"""
        if len(self.full_history) > 50:
            # Extract key discussion points
            topics = []
            for turn in self.full_history[-20:]:
                if "‡∏™‡∏ô‡∏ó‡∏ô‡∏≤" in turn["message"] or "‡∏Å‡∏•‡πà‡∏≤‡∏ß" in turn["message"]:
                    topics.append(turn["message"][:100])
            
            summary = "=== CONVERSATION SUMMARY ===\n"
            summary += f"Total turns: {len(self.full_history)}\n"
            summary += f"Recent topics: {', '.join(topics[:3])}\n"
            summary += "---\n"
            return summary
        return ""
    
    def search_related(self, keyword: str) -> List[Dict]:
        """Search for related conversation turns"""
        results = []
        for turn in self.full_history:
            if keyword.lower() in turn["message"].lower() or keyword.lower() in turn["response"].lower():
                results.append(turn)
        return results[-5:]  # Return last 5 matches
    
    def export(self, filepath: str):
        """Export conversation to JSON"""
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump({
                "total_turns": len(self.full_history),
                "history": self.full_history
            }, f, ensure_ascii=False, indent=2)


class EnhancedAIPlayer:
    """AI Player with memory management"""
    
    def __init__(self, name: str, cfg: Dict[str, Any], memory: ConversationMemory):
        self.name = name
        self.evc = EVCEngine(cfg)
        self.llm = get_llm_core()
        self.cfg = cfg
        self.memory = memory
        self.personality = self._init_personality()
    
    def _init_personality(self) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå AI"""
        personalities = {
            "A": "‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤ ‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• ‡∏ä‡∏≠‡∏ö‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô",
            "B": "‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå ‡∏ô‡∏¥‡∏¢‡∏°‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏ä‡∏≠‡∏ö‡∏™‡∏≥‡∏£‡∏ß‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡πÉ‡∏´‡∏°‡πà"
        }
        return personalities.get(self.name, "‡∏ú‡∏π‡πâ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤")
    
    def respond(self, prompt: str, mode: str = "chat", max_context_turns: int = 20) -> Tuple[str, Dict]:
        """Generate response with full conversation context"""
        
        try:
            # Update EVC from prompt
            evc_info = self.evc.update_from_text(prompt)
            tone = self.evc.tone_from_phase(self.cfg.get("tone_map", {}))
            
            # Build context-aware system prompt
            history_context = self.memory.get_context(max_context_turns)
            summary_context = self.memory.get_summary_context()
            
            system_prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ {self.name} - {self.personality}

‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:
{summary_context}
{history_context}

Tone ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {tone}
EVC Phase: {self.evc._phase()}

‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
- ‡∏à‡∏≥‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
- ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏π‡∏î‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤

‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
            
            if mode == "evc":
                system_prompt += "\n\n‡πÇ‡∏´‡∏°‡∏î‡∏û‡∏¥‡πÄ‡∏®‡∏©: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ï‡∏≤‡∏°‡∏Å‡∏£‡∏≠‡∏ö EVC"
            
            # Generate response WITH context
            raw_response = self.llm.generate(
                prompt=prompt,
                system_prompt=system_prompt,
                mode=mode,
                evc_state={
                    "E": self.evc.E,
                    "K": self.evc.K,
                    "phase": self.evc._phase(),
                    "dE": evc_info.get("dE", 0.0)
                }
            )
            
            if raw_response.startswith("‚ö†Ô∏è"):
                response = f"‚ö†Ô∏è {self.name} - ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠"
            else:
                response = raw_response
            
            # Reflect on response
            try:
                rvec = reflect(prompt, response)
            except:
                rvec = {"coherence": 0.5, "toxicity": 0.0, "satisfaction": 0.5, "verbosity": 0.5}
            
            # Update EVC from reflection
            evc_info2 = self.evc.update_from_reflection(rvec)
            
            # Save to memory
            self.memory.add_turn(self.name, prompt, response, evc_info2)
            
            turn_data = {
                "player": self.name,
                "prompt": prompt,
                "response": response,
                "evc_before": evc_info,
                "evc_after": evc_info2,
                "reflection": rvec,
                "mode": mode,
                "timestamp": datetime.now().isoformat()
            }
            
            return response, turn_data
            
        except Exception as e:
            raise Exception(f"‚ùå {self.name} Error: {str(e)}")


class LongContextConversation:
    """Manage long-running dual AI conversation"""
    
    def __init__(self, cfg: Dict[str, Any], topic: str):
        self.cfg = cfg
        self.topic = topic
        self.memory = ConversationMemory(max_turns=100)
        self.player_a = EnhancedAIPlayer("A", cfg, self.memory)
        self.player_b = EnhancedAIPlayer("B", cfg, self.memory)
        self.episode_data = {
            "topic": topic,
            "timestamp": datetime.now().isoformat(),
            "turns": []
        }
        self.turn_count = 0
    
    def run_long_conversation(self, max_turns: int = 20, 
                             save_every: int = 5,
                             progress_callback = None) -> Dict[str, Any]:
        """Run extended conversation that remembers all context"""
        
        def progress_cb(msg):
            if progress_callback:
                progress_callback(msg)
        
        # Initial prompt
        current_prompt = f"‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏ô‡∏ó‡∏ô‡∏≤: {self.topic}\n\n‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"
        players = [self.player_a, self.player_b]
        
        for turn_idx in range(max_turns):
            current_player = players[turn_idx % 2]
            other_player = players[(turn_idx + 1) % 2]
            
            try:
                # Decide mode (mostly chat, some EVC analysis)
                mode = "evc" if turn_idx % 5 == 0 else "chat"
                
                progress_cb(f"üîÑ Turn {turn_idx + 1}/{max_turns} - {current_player.name} ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤...")
                
                # Get response WITH full context
                response, turn_data = current_player.respond(
                    current_prompt,
                    mode=mode,
                    max_context_turns=15  # Keep last 15 turns in context
                )
                
                # Store turn
                turn_info = {
                    "turn": turn_idx + 1,
                    "speaker": current_player.name,
                    "opponent": other_player.name,
                    "mode": mode,
                    "prompt": current_prompt,
                    "response": response,
                    "evc_speaker": turn_data["evc_after"],
                    "reflection": turn_data["reflection"],
                    "timestamp": datetime.now().isoformat()
                }
                
                self.episode_data["turns"].append(turn_info)
                self.turn_count += 1
                
                progress_cb(
                    f"‚úÖ Turn {turn_idx + 1}: {current_player.name} "
                    f"[E={turn_data['evc_after']['E']:.2f} "
                    f"Phase={turn_data['evc_after']['phase']}]"
                )
                
                # Auto-save every N turns
                if (turn_idx + 1) % save_every == 0:
                    self.memory.export(f"conversation_backup_turn_{turn_idx + 1}.json")
                    progress_cb(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡∏£‡∏≠‡∏á turn {turn_idx + 1}")
                
                # Next prompt is current response
                current_prompt = response
                time.sleep(0.5)
                
            except Exception as e:
                progress_cb(f"‚ö†Ô∏è Turn {turn_idx + 1} Error: {str(e)[:100]}")
                continue
        
        # Finalize
        self.episode_data["summary"] = {
            "total_turns": self.turn_count,
            "memory_size": len(self.memory.full_history),
            "final_evc_a": {
                "E": self.player_a.evc.E,
                "K": self.player_a.evc.K,
                "phase": self.player_a.evc._phase()
            },
            "final_evc_b": {
                "E": self.player_b.evc.E,
                "K": self.player_b.evc.K,
                "phase": self.player_b.evc._phase()
            }
        }
        
        return self.episode_data
    
    def export_conversation(self, filepath: str):
        """Export full conversation with metadata"""
        export_data = {
            "metadata": {
                "topic": self.topic,
                "total_turns": self.turn_count,
                "timestamp": datetime.now().isoformat(),
                "players": {
                    "A": {
                        "personality": self.player_a.personality,
                        "final_evc": {
                            "E": self.player_a.evc.E,
                            "K": self.player_a.evc.K
                        }
                    },
                    "B": {
                        "personality": self.player_b.personality,
                        "final_evc": {
                            "E": self.player_b.evc.E,
                            "K": self.player_b.evc.K
                        }
                    }
                }
            },
            "full_history": self.memory.full_history,
            "episode_turns": self.episode_data["turns"]
        }
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
    
    def get_conversation_summary(self) -> str:
        """Generate readable summary of conversation"""
        summary = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë     CONVERSATION SUMMARY                   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìå ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {self.topic}
üîÑ ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {self.turn_count} ‡∏£‡∏≠‡∏ö
üìÖ ‡πÄ‡∏ß‡∏•‡∏≤: {self.episode_data['timestamp']}

üë§ ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°:
  ‚Ä¢ A: {self.player_a.personality} (E={self.player_a.evc.E:.2f}, Phase={self.player_a.evc._phase()})
  ‚Ä¢ B: {self.player_b.personality} (E={self.player_b.evc.E:.2f}, Phase={self.player_b.evc._phase()})

üìä ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {len(self.memory.full_history)} turns ‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥

üí¨ ‡∏Å‡∏≤‡∏£‡∏≠‡∏†‡∏¥‡∏õ‡∏£‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
"""
        
        # Extract key points
        for i, turn in enumerate(self.memory.full_history[-5:]):
            summary += f"\n  {i+1}. [{turn['speaker']}]: {turn['message'][:80]}..."
        
        summary += "\n\n‚úÖ ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n"
        return summary


# ============================================================
# Streamlit Integration
# ============================================================
def run_long_conversation_session(num_turns: int = 20, 
                                  topic: str = None,
                                  st_placeholder = None) -> Dict[str, Any]:
    """Run long conversation from Streamlit"""
    
    cfg = yaml.safe_load(open("config.yaml", encoding="utf-8"))
    
    if topic is None:
        topic = "‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á EVC ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Traditional LLM"
    
    conv = LongContextConversation(cfg, topic)
    
    def progress_cb(msg):
        if st_placeholder:
            st_placeholder.write(msg)
    
    # Run conversation
    episode = conv.run_long_conversation(
        max_turns=num_turns,
        save_every=5,
        progress_callback=progress_cb
    )
    
    # Export
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    export_file = f"data/long_conversation_{timestamp}.json"
    os.makedirs("data", exist_ok=True)
    conv.export_conversation(export_file)
    
    if st_placeholder:
        st_placeholder.success(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏µ‡πà: {export_file}")
        st_placeholder.info(conv.get_conversation_summary())
    
    return {
        "episode": episode,
        "export_file": export_file,
        "memory_size": len(conv.memory.full_history)
    }


if __name__ == "__main__":
    print("üöÄ Starting Long Context Dual AI Conversation...\n")
    
    result = run_long_conversation_session(
        num_turns=10,
        topic="EVC Framework vs Traditional AI: Which is Better?"
    )
    
    print(f"\n‚úÖ Conversation completed!")
    print(f"üìä Total turns in memory: {result['memory_size']}")
    print(f"üíæ Saved to: {result['export_file']}")
    """
FIX: Enhanced Context System Prompt Builder
- ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ AI ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á Turn ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
- ‡πÉ‡∏ä‡πâ XML tags ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô continuity
"""

class FixedContextBuilder:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á system prompt ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô continuity"""
    
    @staticmethod
    def build_context_prompt(
        conversation_history: list,  # ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
        current_turn: int,
        ai_name: str,
        user_query: str,
        evc_state: dict,
        mode: str = "chat"
    ) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á system prompt ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
        """
        
        # ===== 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Formatted History =====
        history_xml = FixedContextBuilder._format_history_xml(
            conversation_history,
            last_n_turns=10  # ‡∏î‡∏∂‡∏á 10 turn ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        )
        
        # ===== 2. ‡∏´‡∏≤ Turn ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ =====
        prev_turn_summary = ""
        if len(conversation_history) > 0:
            last_turn = conversation_history[-1]
            prev_turn_summary = f"""
<previous_turn>
  <turn_number>{current_turn - 1}</turn_number>
  <user_said>{last_turn.get('user_query', '')[:200]}</user_said>
  <your_response_was>{last_turn.get('ai_response', '')[:300]}</your_response_was>
  <key_point>{last_turn.get('key_point', 'N/A')}</key_point>
</previous_turn>
"""
        
        # ===== 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Mandatory Instructions =====
        mandatory_instructions = """
<MANDATORY_INSTRUCTIONS>
üî¥ **CRITICAL** - ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á:**

1. **‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô**
   - ‡∏û‡∏π‡∏î‡∏ß‡πà‡∏≤ "‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß..."
   - ‡∏û‡∏π‡∏î‡∏ß‡πà‡∏≤ "‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á..."
   - ‡∏û‡∏π‡∏î‡∏ß‡πà‡∏≤ "‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏°‡∏ö‡∏≠‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß..."

2. **‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó**
   - ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏û‡∏π‡∏î‡∏ß‡πà‡∏≤ "NVIDIA ‡∏≠‡∏∞" ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å Turn 4
   - ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
   - ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏ß‡πÅ‡∏´‡∏•‡∏° "‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß..."

3. **‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡∏û‡∏π‡∏î**
   - "‡∏ú‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏≤‡∏°..."
   - ‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏π‡∏î‡∏ß‡πà‡∏≤ "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ö‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î" ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß!

4. **‡∏¢‡∏∂‡∏î‡∏´‡∏•‡∏±‡∏Å: Continuity > ‡∏î‡∏π‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°**
   - ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏Å‡∏ï‡∏¥‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

</MANDATORY_INSTRUCTIONS>
"""
        
        # ===== 4. EVC Personality Modifier =====
        phase_instruction = {
            "calm": "‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡πà‡∏ô",
            "focus": "‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏•‡∏≥‡πÄ‡∏´‡∏•‡∏ß",
            "overheat": "‡πÉ‡∏à‡πÄ‡∏¢‡πá‡∏ô ‡∏Ç‡∏≠‡πÇ‡∏ó‡∏© ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à",
            "fear": "‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ä‡πâ‡∏≤‡πÜ",
            "cooldown": "‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô ‡πÄ‡∏î‡∏¥‡∏ô‡∏´‡∏ô‡πâ‡∏≤"
        }.get(evc_state.get('phase', 'calm'), "‡∏ï‡∏≠‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥")
        
        evc_modifier = f"""
<evc_state>
  E={evc_state.get('E', 0.5):.2f}
  K={evc_state.get('K', 0.45):.2f}
  Phase={evc_state.get('phase', 'calm')}
  ‚Üí Tone: {phase_instruction}
</evc_state>
"""
        
        # ===== 5. ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î =====
        system_prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ {ai_name} - ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÑ‡∏î‡πâ‡πÅ‡∏•‡∏∞‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á

{mandatory_instructions}

{evc_modifier}

{'üìä CONVERSATION HISTORY (‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤):' if history_xml else ''}
{history_xml}

{prev_turn_summary}

{'‚ö†Ô∏è MODE: EVC Analysis - ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ï‡∏≤‡∏°‡∏Å‡∏£‡∏≠‡∏ö EVC' if mode == 'evc' else ''}

---

**‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô Turn {current_turn}:**
üßç ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {user_query}

üìç **‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:** ‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡πÑ‡∏°‡πà‡πÉ‡∏´‡∏°‡πà!
"""
        
        return system_prompt
    
    @staticmethod
    def _format_history_xml(conversation_history: list, last_n_turns: int = 10) -> str:
        """
        ‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏õ‡πá‡∏ô XML format ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        """
        if not conversation_history:
            return ""
        
        recent = conversation_history[-last_n_turns:]
        xml = "<conversation_history>\n"
        
        for i, turn in enumerate(recent, 1):
            turn_num = turn.get('turn_number', i)
            user_q = turn.get('user_query', '').strip()[:150]
            ai_resp = turn.get('ai_response', '').strip()[:200]
            key_pt = turn.get('key_point', '')
            
            xml += f"""
  <turn number="{turn_num}">
    <user>{user_q}</user>
    <assistant>{ai_resp}</assistant>
    <theme>{key_pt}</theme>
  </turn>
"""
        
        xml += "</conversation_history>\n"
        return xml


# ============================================================
# Integration with core_llm.py
# ============================================================

class ContextAwareLLM:
    """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç core_llm.py ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ context ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"""
    
    def __init__(self, base_llm_instance):
        self.llm = base_llm_instance
        self.conversation_history = []
    
    def add_turn(self, user_query: str, ai_response: str, key_point: str = ""):
        """‡πÄ‡∏û‡∏¥‡πà‡∏° turn ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥"""
        self.conversation_history.append({
            "turn_number": len(self.conversation_history) + 1,
            "user_query": user_query,
            "ai_response": ai_response,
            "key_point": key_point
        })
    
    def generate_with_context(
        self,
        user_query: str,
        ai_name: str = "Assistant",
        evc_state: dict = None,
        mode: str = "chat"
    ) -> str:
        """
        ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö context ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        """
        
        if evc_state is None:
            evc_state = {"E": 0.5, "K": 0.45, "phase": "calm"}
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á context prompt ‡∏ó‡∏µ‡πà‡∏î‡∏µ
        system_prompt = FixedContextBuilder.build_context_prompt(
            conversation_history=self.conversation_history,
            current_turn=len(self.conversation_history) + 1,
            ai_name=ai_name,
            user_query=user_query,
            evc_state=evc_state,
            mode=mode
        )
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        response = self.llm.generate(
            prompt=user_query,
            system_prompt=system_prompt,
            mode=mode,
            evc_state=evc_state
        )
        
        # ‡∏´‡∏≤ key point
        key_point = self._extract_key_point(user_query, response)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å turn
        self.add_turn(user_query, response, key_point)
        
        return response
    
    @staticmethod
    def _extract_key_point(query: str, response: str) -> str:
        """‡∏î‡∏∂‡∏á key point ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"""
        # Simple: ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á response
        first_sentence = response.split('\n')[0]
        return first_sentence[:100] if first_sentence else "General discussion"


# ============================================================
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
# ============================================================

if __name__ == "__main__":
    from core_llm import get_llm_core
    import json
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á context-aware wrapper
    base_llm = get_llm_core()
    context_llm = ContextAwareLLM(base_llm)
    
    # Simulate conversation
    evc_state = {"E": 0.5, "K": 0.45, "phase": "calm"}
    
    # Turn 1
    q1 = "NVIDIA ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"
    r1 = context_llm.generate_with_context(q1, evc_state=evc_state)
    print(f"Turn 1\nüßç Q: {q1}\nü§ñ A: {r1[:200]}\n")
    
    # Turn 2 - ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å Turn 1
    q2 = "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á GPU ‡∏ó‡∏µ‡πà NVIDIA ‡∏ó‡∏≥"
    r2 = context_llm.generate_with_context(q2, evc_state=evc_state)
    print(f"Turn 2\nüßç Q: {q2}\nü§ñ A: {r2[:200]}\n")
    
    # Turn 3
    q3 = "‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£?"
    r3 = context_llm.generate_with_context(q3, evc_state=evc_state)
    print(f"Turn 3\nüßç Q: {q3}\nü§ñ A: {r3[:200]}\n")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    history_data = {
        "turns": context_llm.conversation_history,
        "total": len(context_llm.conversation_history)
    }
    
    print("\n=== FULL HISTORY ===")
    print(json.dumps(history_data, ensure_ascii=False, indent=2))
    
    print(f"\n‚úÖ Turn ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô!")
    print(f"üìä ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(context_llm.conversation_history)} turns ‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥")